{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T08:59:03.244729Z",
     "start_time": "2021-02-17T08:59:02.227726Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from utils import parse_for_tables, preprocess\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T08:59:03.657034Z",
     "start_time": "2021-02-17T08:59:03.614041Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'robako gasstraßenbeleuchtung gmbh berlin jahresabschluss zum geschäftsjahr vom 01.01.2008 bis zum 31.12.2008 bilanz aktiva 31.12.2008 eur a. anlagevermögen 2.537,48 i. sachanlagen 37,48 1. andere anlagen, betriebs und geschäftsausstattung 37,48 ii. finanzanlagen 2.500,00 1. sonstige ausleihungen 2.500,00 b. umlaufvermögen 29.120,72 i. forderungen und sonstige vermögensgegenstände 28.844,22 1. forderungen aus lieferungen und leistungen 28.397,35 2. sonstige vermögensgegenstände 446,87 ii. kassenbestand, bundesbankguthaben, guthaben bei kreditinstituten und schecks 276,50 bilanzsumme, summe aktiva 31.658,20 passiva 31.12.2008 eur a. eigenkapital 13.161,23 i. gezeichnetes kapital 25.564,59 ii. verlustvortrag 15.780,67 iii. jahresüberschuss 3.377,31 b. verbindlichkeiten 18.496,97 1. verbindlichkeiten gegenüber kreditinstituten 8.964,02 davon mit einer restlaufzeit bis zu einem jahr 8.964,02 2. verbindlichkeiten aus lieferungen und leistungen 2.762,29 davon mit einer restlaufzeit bis zu einem jahr 2.762,29 3. sonstige verbindlichkeiten 6.770,66 davon aus steuern 3.023,94 davon mit einer restlaufzeit bis zu einem jahr 6.770,66 bilanzsumme, summe passiva 31.658,20'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = parse_for_tables(\"./ML Task/balance_sheets_examples/random_cases/0.html\")\n",
    "data = tables[0]\n",
    "data = preprocess(data)\n",
    "data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T08:59:03.864162Z",
     "start_time": "2021-02-17T08:59:03.853165Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_de(data):\n",
    "    doc = nlp.tokenizer(data)\n",
    "    tokenized_data = [tok.text for tok in doc]\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T08:59:04.119239Z",
     "start_time": "2021-02-17T08:59:04.104240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robako',\n",
       " 'gasstraßenbeleuchtung',\n",
       " 'gmbh',\n",
       " 'berlin',\n",
       " 'jahresabschluss',\n",
       " 'zum',\n",
       " 'geschäftsjahr',\n",
       " 'vom',\n",
       " '01.01.2008',\n",
       " 'bis',\n",
       " 'zum',\n",
       " '31.12.2008',\n",
       " 'bilanz',\n",
       " 'aktiva',\n",
       " '31.12.2008',\n",
       " 'eur',\n",
       " 'a.',\n",
       " 'anlagevermögen',\n",
       " '2.537,48',\n",
       " 'i.',\n",
       " 'sachanlagen',\n",
       " '37,48',\n",
       " '1.',\n",
       " 'andere',\n",
       " 'anlagen',\n",
       " ',',\n",
       " 'betriebs',\n",
       " 'und',\n",
       " 'geschäftsausstattung',\n",
       " '37,48',\n",
       " 'ii',\n",
       " '.',\n",
       " 'finanzanlagen',\n",
       " '2.500,00',\n",
       " '1.',\n",
       " 'sonstige',\n",
       " 'ausleihungen',\n",
       " '2.500,00',\n",
       " 'b.',\n",
       " 'umlaufvermögen',\n",
       " '29.120,72',\n",
       " 'i.',\n",
       " 'forderungen',\n",
       " 'und',\n",
       " 'sonstige',\n",
       " 'vermögensgegenstände',\n",
       " '28.844,22',\n",
       " '1.',\n",
       " 'forderungen',\n",
       " 'aus',\n",
       " 'lieferungen',\n",
       " 'und',\n",
       " 'leistungen',\n",
       " '28.397,35',\n",
       " '2.',\n",
       " 'sonstige',\n",
       " 'vermögensgegenstände',\n",
       " '446,87',\n",
       " 'ii',\n",
       " '.',\n",
       " 'kassenbestand',\n",
       " ',',\n",
       " 'bundesbankguthaben',\n",
       " ',',\n",
       " 'guthaben',\n",
       " 'bei',\n",
       " 'kreditinstituten',\n",
       " 'und',\n",
       " 'schecks',\n",
       " '276,50',\n",
       " 'bilanzsumme',\n",
       " ',',\n",
       " 'summe',\n",
       " 'aktiva',\n",
       " '31.658,20',\n",
       " 'passiva',\n",
       " '31.12.2008',\n",
       " 'eur',\n",
       " 'a.',\n",
       " 'eigenkapital',\n",
       " '13.161,23',\n",
       " 'i.',\n",
       " 'gezeichnetes',\n",
       " 'kapital',\n",
       " '25.564,59',\n",
       " 'ii',\n",
       " '.',\n",
       " 'verlustvortrag',\n",
       " '15.780,67',\n",
       " 'iii',\n",
       " '.',\n",
       " 'jahresüberschuss',\n",
       " '3.377,31',\n",
       " 'b.',\n",
       " 'verbindlichkeiten',\n",
       " '18.496,97',\n",
       " '1.',\n",
       " 'verbindlichkeiten',\n",
       " 'gegenüber',\n",
       " 'kreditinstituten',\n",
       " '8.964,02',\n",
       " 'davon',\n",
       " 'mit',\n",
       " 'einer',\n",
       " 'restlaufzeit',\n",
       " 'bis',\n",
       " 'zu',\n",
       " 'einem',\n",
       " 'jahr',\n",
       " '8.964,02',\n",
       " '2.',\n",
       " 'verbindlichkeiten',\n",
       " 'aus',\n",
       " 'lieferungen',\n",
       " 'und',\n",
       " 'leistungen',\n",
       " '2.762,29',\n",
       " 'davon',\n",
       " 'mit',\n",
       " 'einer',\n",
       " 'restlaufzeit',\n",
       " 'bis',\n",
       " 'zu',\n",
       " 'einem',\n",
       " 'jahr',\n",
       " '2.762,29',\n",
       " '3.',\n",
       " 'sonstige',\n",
       " 'verbindlichkeiten',\n",
       " '6.770,66',\n",
       " 'davon',\n",
       " 'aus',\n",
       " 'steuern',\n",
       " '3.023,94',\n",
       " 'davon',\n",
       " 'mit',\n",
       " 'einer',\n",
       " 'restlaufzeit',\n",
       " 'bis',\n",
       " 'zu',\n",
       " 'einem',\n",
       " 'jahr',\n",
       " '6.770,66',\n",
       " 'bilanzsumme',\n",
       " ',',\n",
       " 'summe',\n",
       " 'passiva',\n",
       " '31.658,20']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_de(data.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:38.401568Z",
     "start_time": "2021-02-17T09:00:38.384567Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(\"./dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:38.828054Z",
     "start_time": "2021-02-17T09:00:38.818054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robako gasstraßenbeleuchtung gmbh berlin jahre...</td>\n",
       "      <td>aktiva : anlagevermögen : 2.537,48 aktiva : sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transporte bauer gmbh bockhorn jahresabschluss...</td>\n",
       "      <td>aktiva : umlaufvermögen : 25.023,63 | aktiva :...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  robako gasstraßenbeleuchtung gmbh berlin jahre...   \n",
       "1  transporte bauer gmbh bockhorn jahresabschluss...   \n",
       "\n",
       "                                                 trg  \n",
       "0  aktiva : anlagevermögen : 2.537,48 aktiva : sa...  \n",
       "1  aktiva : umlaufvermögen : 25.023,63 | aktiva :...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:39.226327Z",
     "start_time": "2021-02-17T09:00:39.218329Z"
    }
   },
   "outputs": [],
   "source": [
    "field_ = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "fields = [('src', field_), ('trg', field_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:39.560564Z",
     "start_time": "2021-02-17T09:00:39.544565Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = TabularDataset.splits(\n",
    "                                        path = './',\n",
    "                                        train = \"dataset.csv\",\n",
    "                                        validation = \"dataset.csv\",\n",
    "                                        test = \"dataset.csv\",\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:39.976127Z",
     "start_time": "2021-02-17T09:00:39.969130Z"
    }
   },
   "outputs": [],
   "source": [
    "field_.build_vocab(train_data, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:40.817548Z",
     "start_time": "2021-02-17T09:00:40.805549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x000001A09FC87C08>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             '<sos>': 2,\n",
       "             '<eos>': 3,\n",
       "             ':': 4,\n",
       "             '|': 5,\n",
       "             'passiva': 6,\n",
       "             'verbindlichkeiten': 7,\n",
       "             'aktiva': 8,\n",
       "             'und': 9,\n",
       "             'sonstige': 10,\n",
       "             ',': 11,\n",
       "             'eigenkapital': 12,\n",
       "             'umlaufvermögen': 13,\n",
       "             'bis': 14,\n",
       "             'davon': 15,\n",
       "             'forderungen': 16,\n",
       "             'vermögensgegenstände': 17,\n",
       "             'aus': 18,\n",
       "             'kreditinstituten': 19,\n",
       "             '25.023,63': 20,\n",
       "             'einem': 21,\n",
       "             'einer': 22,\n",
       "             'jahr': 23,\n",
       "             'mit': 24,\n",
       "             'restlaufzeit': 25,\n",
       "             'zu': 26,\n",
       "             '.': 27,\n",
       "             'i.': 28,\n",
       "             'leistungen': 29,\n",
       "             'lieferungen': 30,\n",
       "             '1.': 31,\n",
       "             '2.500,00': 32,\n",
       "             '2.762,29': 33,\n",
       "             '31.658,20': 34,\n",
       "             '37,48': 35,\n",
       "             '8.964,02': 36,\n",
       "             'a.': 37,\n",
       "             'bei': 38,\n",
       "             'bilanzsumme': 39,\n",
       "             'bundesbankguthaben': 40,\n",
       "             'eur': 41,\n",
       "             'gezeichnetes': 42,\n",
       "             'guthaben': 43,\n",
       "             'ii': 44,\n",
       "             'kapital': 45,\n",
       "             'kassenbestand': 46,\n",
       "             'schecks': 47,\n",
       "             'summe': 48,\n",
       "             'total': 49,\n",
       "             'zum': 50,\n",
       "             '28.397,35': 51,\n",
       "             '3.023,94': 52,\n",
       "             '31.12.2008': 53,\n",
       "             '31.12.2010': 54,\n",
       "             '6.770,66': 55,\n",
       "             'b.': 56,\n",
       "             'finanzanlagen': 57,\n",
       "             'gegenüber': 58,\n",
       "             'sachanlagen': 59,\n",
       "             ' ': 60,\n",
       "             '13.161,23': 61,\n",
       "             '15.780,67': 62,\n",
       "             '2.': 63,\n",
       "             '2.537,48': 64,\n",
       "             '24.189,25': 65,\n",
       "             '24.956,35': 66,\n",
       "             '25.000,00': 67,\n",
       "             '25.564,59': 68,\n",
       "             '276,50': 69,\n",
       "             '28.844,22': 70,\n",
       "             '29.120,72': 71,\n",
       "             '3.377,31': 72,\n",
       "             '400,00': 73,\n",
       "             '434,38': 74,\n",
       "             '67,28': 75,\n",
       "             '810,75': 76,\n",
       "             'andere': 77,\n",
       "             'anlagen': 78,\n",
       "             'anlagevermögen': 79,\n",
       "             'ausleihungen': 80,\n",
       "             'betriebs': 81,\n",
       "             'bilanz': 82,\n",
       "             'geschäftsausstattung': 83,\n",
       "             'geschäftsjahr': 84,\n",
       "             'gmbh': 85,\n",
       "             'jahresabschluss': 86,\n",
       "             'jahresfehlbetrag': 87,\n",
       "             'jahresüberschuss': 88,\n",
       "             'rückstellungen': 89,\n",
       "             'steuern': 90,\n",
       "             'verlustvortrag': 91,\n",
       "             'vom': 92,\n",
       "             '||': 93,\n",
       "             '01.01.2008': 94,\n",
       "             '01.01.2010': 95,\n",
       "             '18.496,97': 96,\n",
       "             '3.': 97,\n",
       "             '446,87': 98,\n",
       "             'bauer': 99,\n",
       "             'berlin': 100,\n",
       "             'bockhorn': 101,\n",
       "             'c.': 102,\n",
       "             'gasstraßenbeleuchtung': 103,\n",
       "             'ii.': 104,\n",
       "             'iii': 105,\n",
       "             'robako': 106,\n",
       "             'transporte': 107})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:41.406547Z",
     "start_time": "2021-02-17T09:00:41.356549Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:42.302536Z",
     "start_time": "2021-02-17T09:00:42.289538Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:43.517525Z",
     "start_time": "2021-02-17T09:00:43.505530Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:00:44.040148Z",
     "start_time": "2021-02-17T09:00:44.017151Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:22.740137Z",
     "start_time": "2021-02-17T09:05:22.727138Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [0, 1, 2, 3, ..., src len - 1]\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, src len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, src len]\n",
    "        \n",
    "        #begin convolutional blocks...\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input))\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, src len]\n",
    "\n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "        \n",
    "        #...end convolutional blocks\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved = [batch size, src len, emb dim]\n",
    "        \n",
    "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:23.228606Z",
     "start_time": "2021-02-17T09:05:23.207608Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 trg_pad_idx, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        #conved = [batch size, hid dim, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved_emb = [batch size, trg len, emb dim]\n",
    "        \n",
    "        combined = (conved_emb + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, trg len, emb dim]\n",
    "                \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        \n",
    "        #energy = [batch size, trg len, src len]\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        \n",
    "        #attention = [batch size, trg len, src len]\n",
    "            \n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, emd dim]\n",
    "        \n",
    "        #convert from emb dim -> hid dim\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #apply residual connection\n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        \n",
    "        #attended_combined = [batch size, hid dim, trg len]\n",
    "        \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "            \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, trg len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = [batch size, trg len, emb dim]\n",
    "        #pos_embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, trg len]\n",
    "        \n",
    "        batch_size = conv_input.shape[0]\n",
    "        hid_dim = conv_input.shape[1]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #apply dropout\n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            #need to pad so decoder can't \"cheat\"\n",
    "            padding = torch.zeros(batch_size, \n",
    "                                  hid_dim, \n",
    "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
    "                \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
    "        \n",
    "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(padded_conv_input)\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, trg len]\n",
    "            \n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #calculate attention\n",
    "            attention, conved = self.calculate_attention(embedded, \n",
    "                                                         conved, \n",
    "                                                         encoder_conved, \n",
    "                                                         encoder_combined)\n",
    "            \n",
    "            #attention = [batch size, trg len, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "         \n",
    "        #conved = [batch size, trg len, emb dim]\n",
    "            \n",
    "        output = self.fc_out(self.dropout(conved))\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:23.595208Z",
     "start_time": "2021-02-17T09:05:23.585212Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n",
    "           \n",
    "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
    "        #encoder_conved is output from final encoder conv. block\n",
    "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
    "        #  positional embeddings \n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "            \n",
    "        #encoder_conved = [batch size, src len, emb dim]\n",
    "        #encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #calculate predictions of next words\n",
    "        #output is a batch of predictions for each word in the trg sentence\n",
    "        #attention a batch of attention scores across the src sentence for \n",
    "        #  each word in the trg sentence\n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #attention = [batch size, trg len - 1, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:24.513709Z",
     "start_time": "2021-02-17T09:05:24.160708Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(field_.vocab)\n",
    "OUTPUT_DIM = len(field_.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
    "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
    "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
    "ENC_KERNEL_SIZE = 3 # must be odd!\n",
    "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "MAX_LEN = 300\n",
    "TRG_PAD_IDX = field_.vocab.stoi[field_.pad_token]\n",
    "    \n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device, MAX_LEN)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device, MAX_LEN)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:25.001189Z",
     "start_time": "2021-02-17T09:05:24.987188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 32,503,148 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:25.508594Z",
     "start_time": "2021-02-17T09:05:25.491599Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:25.966295Z",
     "start_time": "2021-02-17T09:05:25.950298Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:26.405031Z",
     "start_time": "2021-02-17T09:05:26.392034Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:26.915591Z",
     "start_time": "2021-02-17T09:05:26.911589Z"
    }
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:05:43.544582Z",
     "start_time": "2021-02-17T09:05:27.747582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 4.655 | Train PPL: 105.155\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 4.125 | Train PPL:  61.875\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 4.602 | Train PPL:  99.676\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 3.048 | Train PPL:  21.083\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 2.903 | Train PPL:  18.233\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 2.616 | Train PPL:  13.688\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 2.459 | Train PPL:  11.689\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 2.261 | Train PPL:   9.593\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 2.126 | Train PPL:   8.384\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 2.129 | Train PPL:   8.406\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 1.970 | Train PPL:   7.168\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 1.780 | Train PPL:   5.927\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 1.710 | Train PPL:   5.528\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 1.750 | Train PPL:   5.753\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 1.380 | Train PPL:   3.977\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 1.323 | Train PPL:   3.755\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 1.290 | Train PPL:   3.633\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 1.188 | Train PPL:   3.280\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 1.052 | Train PPL:   2.864\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 1.583 | Train PPL:   4.870\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 1.279 | Train PPL:   3.593\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 1.173 | Train PPL:   3.230\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 1.212 | Train PPL:   3.359\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 1.477 | Train PPL:   4.379\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 1.303 | Train PPL:   3.679\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 1.537 | Train PPL:   4.651\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 1.550 | Train PPL:   4.713\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 1.931 | Train PPL:   6.894\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 3.353 | Train PPL:  28.575\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 2.635 | Train PPL:  13.947\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 5.276 | Train PPL: 195.670\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 3.657 | Train PPL:  38.745\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 4.123 | Train PPL:  61.757\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 4.414 | Train PPL:  82.574\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 5.221 | Train PPL: 185.164\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 4.336 | Train PPL:  76.372\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 7.059 | Train PPL: 1163.250\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 15.378 | Train PPL: 4768327.899\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 15.384 | Train PPL: 4797241.377\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 15.720 | Train PPL: 6717837.315\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 19.254 | Train PPL: 230184269.199\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 8.306 | Train PPL: 4048.357\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 5.732 | Train PPL: 308.476\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 3.572 | Train PPL:  35.583\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 3.193 | Train PPL:  24.369\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 3.092 | Train PPL:  22.031\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 11.902 | Train PPL: 147595.294\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 10.028 | Train PPL: 22654.318\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 6.969 | Train PPL: 1063.539\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 6.320 | Train PPL: 555.647\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 5.082 | Train PPL: 161.094\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 2.730 | Train PPL:  15.326\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 2.516 | Train PPL:  12.385\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 2.545 | Train PPL:  12.743\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 2.661 | Train PPL:  14.317\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 2.509 | Train PPL:  12.289\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 2.858 | Train PPL:  17.418\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 3.661 | Train PPL:  38.884\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 3.746 | Train PPL:  42.353\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 4.438 | Train PPL:  84.625\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 6.880 | Train PPL: 972.722\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 5.294 | Train PPL: 199.116\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 6.346 | Train PPL: 570.395\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 8.461 | Train PPL: 4727.411\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 6.395 | Train PPL: 598.900\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 4.053 | Train PPL:  57.588\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 2.872 | Train PPL:  17.664\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 2.920 | Train PPL:  18.547\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 2.664 | Train PPL:  14.353\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 2.262 | Train PPL:   9.606\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 3.829 | Train PPL:  46.022\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 2.410 | Train PPL:  11.133\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 1.848 | Train PPL:   6.349\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 1.995 | Train PPL:   7.355\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 2.093 | Train PPL:   8.109\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 1.945 | Train PPL:   6.996\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 5.567 | Train PPL: 261.572\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 2.844 | Train PPL:  17.188\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 2.297 | Train PPL:   9.942\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 2.369 | Train PPL:  10.685\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 2.062 | Train PPL:   7.858\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 1.890 | Train PPL:   6.622\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 2.009 | Train PPL:   7.455\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 1.906 | Train PPL:   6.728\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 1.828 | Train PPL:   6.224\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 1.584 | Train PPL:   4.876\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 1.650 | Train PPL:   5.207\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 1.421 | Train PPL:   4.142\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 1.484 | Train PPL:   4.409\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 1.352 | Train PPL:   3.865\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 1.247 | Train PPL:   3.481\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 1.192 | Train PPL:   3.293\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 1.063 | Train PPL:   2.896\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 1.117 | Train PPL:   3.055\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 1.101 | Train PPL:   3.007\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 1.017 | Train PPL:   2.764\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.927 | Train PPL:   2.528\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.879 | Train PPL:   2.408\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.821 | Train PPL:   2.272\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.680 | Train PPL:   1.974\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.716 | Train PPL:   2.046\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.736 | Train PPL:   2.087\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.699 | Train PPL:   2.012\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.568 | Train PPL:   1.765\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.500 | Train PPL:   1.649\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.648 | Train PPL:   1.913\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.621\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.966 | Train PPL:   2.628\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 2.209 | Train PPL:   9.111\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.917 | Train PPL:   2.502\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.703 | Train PPL:   2.019\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.784 | Train PPL:   2.190\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.682 | Train PPL:   1.978\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.586 | Train PPL:   1.797\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.627\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.617 | Train PPL:   1.854\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.552 | Train PPL:   1.736\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.691\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.773\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.727 | Train PPL:   2.068\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.970 | Train PPL:   2.638\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 18.544 | Train PPL: 113076732.422\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 11.037 | Train PPL: 62109.633\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 8.669 | Train PPL: 5819.303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.884 | Train PPL:   2.420\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.652 | Train PPL:   1.920\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.497\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 1.815 | Train PPL:   6.140\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 5.051 | Train PPL: 156.247\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 8.645 | Train PPL: 5683.220\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 1.178 | Train PPL:   3.248\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.711 | Train PPL:   2.036\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.952 | Train PPL:   2.592\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.601\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 200\n",
    "CLIP = 0.1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:06:57.087447Z",
     "start_time": "2021-02-17T09:06:57.072447Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_conved, encoder_combined = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:07:16.976917Z",
     "start_time": "2021-02-17T09:07:16.972916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['transporte', 'bauer', 'gmbh', 'bockhorn', 'jahresabschluss', 'zum', 'geschäftsjahr', 'vom', '01.01.2010', 'bis', 'zum', '31.12.2010', 'bilanz', 'aktiva', '31.12.2010', 'eur', 'a.', 'umlaufvermögen', '25.023,63', 'i.', 'forderungen', 'und', 'sonstige', 'vermögensgegenstände', '67,28', 'ii', '.', 'kassenbestand', ',', 'bundesbankguthaben', ',', 'guthaben', 'bei', 'kreditinstituten', 'und', 'schecks', '24.956,35', 'bilanzsumme', ',', 'summe', 'aktiva', '25.023,63', 'passiva', '31.12.2010', 'eur', 'a.', 'eigenkapital', '24.189,25', 'i.', 'gezeichnetes', 'kapital', '25.000,00', 'ii.', 'jahresfehlbetrag', '810,75', 'b.', 'rückstellungen', '400,00', 'c.', 'verbindlichkeiten', '434,38', 'bilanzsumme', ',', 'summe', 'passiva', '25.023,63']\n",
      "trg = ['aktiva', ':', 'umlaufvermögen', ':', '25.023,63', '|', 'aktiva', ':', 'umlaufvermögen', ':', 'forderungen', 'und', 'sonstige', 'vermögensgegenstände', ':', '67,28', '|', 'aktiva', ':', 'umlaufvermögen', ':', 'kassenbestand', 'bundesbankguthaben', 'guthaben', 'bei', 'kreditinstituten', 'und', 'schecks', ':', '24.956,35', '|', 'aktiva', ':', 'total', ':', '25.023,63', '||', 'passiva', ':', 'eigenkapital', ':', '24.189,25', '|', 'passiva', ':', 'eigenkapital', ' ', ':', 'gezeichnetes', 'kapital', ':', '25.000,00', '|', 'passiva', ':', 'eigenkapital', ' ', ':', 'jahresfehlbetrag', ':', '810,75', '|', 'passiva', ':', 'rückstellungen', ':', '400,00', '|', 'passiva', ':', 'verbindlichkeiten', ':', '434,38', '|', 'passiva', ':', 'total', ':', '25.023,63']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 1\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:08:16.276444Z",
     "start_time": "2021-02-17T09:08:13.415443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['aktiva', ':', 'anlagevermögen', ':', '2.537,48', 'aktiva', ':', 'sachanlagen', ':', '37,48', '|', 'aktiva', ':', 'sachanlagen', ':', 'andere', 'anlagen', 'betriebs', 'und', 'geschäftsausstattung', ':', '37,48', '|', 'aktiva', ':', 'finanzanlagen', ':', '2.500,00', '|', 'aktiva', ':', 'finanzanlagen', ':', 'sonstige', 'ausleihungen', ':', '2.500,00', '|', 'aktiva', ':', 'umlaufvermögen', ':', '29.120,72', '|', 'aktiva', ':', 'umlaufvermögen', ':', 'forderungen', 'und', 'sonstige', 'vermögensgegenstände', ':', '28.844,22', '|', 'aktiva', ':', 'umlaufvermögen', ':', 'forderungen', 'und', 'sonstige', 'vermögensgegenstände', ':', 'forderungen', 'aus', 'lieferungen', 'und', 'leistungen', ':', '28.397,35', '|', 'aktiva', ':', 'umlaufvermögen', ':', 'forderungen', 'und', 'sonstige', 'vermögensgegenstände', ':', 'sonstige', 'vermögensgegenstände', ':', '28.397,35', '|', 'aktiva', ':', 'kassenbestand', 'bundesbankguthaben', 'guthaben', 'bei', 'kreditinstituten', 'und', 'schecks', ':', '276,50', '|', 'aktiva', ':', 'total', ':', '31.658,20', '||', 'passiva', ':', 'eigenkapital', ':', '13.161,23', '|', 'passiva', ':', 'eigenkapital', ':', 'gezeichnetes', 'kapital', ':', '25.564,59', '|', 'passiva', ':', 'eigenkapital', ':', 'verlustvortrag', ':', '15.780,67', '|', 'passiva', ':', 'eigenkapital', ':', 'jahresüberschuss', ':', '3.377,31', '|', 'passiva', ':', 'verbindlichkeiten', ':', 'verbindlichkeiten', 'gegenüber', 'kreditinstituten', ':', '8.964,02', '|', 'passiva', ':', 'verbindlichkeiten', ':', 'verbindlichkeiten', 'gegenüber', 'kreditinstituten', ':', 'davon', 'mit', 'einer', 'restlaufzeit', 'bis', 'zu', 'einem', 'jahr', ':', '8.964,02', '|', 'passiva', ':', 'verbindlichkeiten', ':', 'verbindlichkeiten', 'aus', 'lieferungen', 'und', 'leistungen', ':', '2.762,29', '|', 'passiva', ':', 'verbindlichkeiten', ':', 'verbindlichkeiten', 'aus', 'lieferungen', 'und', 'leistungen', ':', 'davon', 'mit', 'einer', 'restlaufzeit', 'bis', 'zu', 'einem', 'jahr', ':', '2.762,29', '|', 'passiva', ':', 'verbindlichkeiten', ':', 'sonstige', 'verbindlichkeiten', ':', '6.770,66', '|', 'passiva', ':', 'verbindlichkeiten', ':', 'sonstige', 'verbindlichkeiten', ':', 'davon', 'aus', 'steuern', ':', '3.023,94', '|', 'passiva', ':', 'verbindlichkeiten', ':', 'sonstige', 'verbindlichkeiten', ':', 'davon', 'mit', 'einer', 'restlaufzeit', 'bis', 'zu', 'einem', 'jahr', ':', '3.023,94', '|', 'passiva', ':', 'total', ':', '31.658,20', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, field_, field_, model, device, max_len = MAX_LEN)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:10:10.913919Z",
     "start_time": "2021-02-17T09:10:10.868920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arcus planung + beratung gmbh & co. service kg cottbus jahresabschluss zum geschäftsjahr vom 01.01.2011 bis zum 31.12.2011 bilanz zum 31. dezember 2011 aktiva geschäftsjahr eur vorjahr eur a. umlaufvermögen i. forderungen und sonstige vermögensgegenstände 135.437,50 117.884,92 ii. kassenbestand, bundesbankguthaben, guthaben bei kreditinstituten und schecks 3.764,17 7.699,61 139.201,67 125.584,53 passiva geschäftsjahr eur vorjahr eur a. eigenkapital i. kapitalanteile kommanditisten 12.000,00 12.000,00 b. rückstellungen 19.153,34 22.175,60 c. verbindlichkeiten 108.048,33 91.408,93 139.201,67 125.584,53'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = parse_for_tables(\"./ML Task/2011.html\")\n",
    "data = tables[0]\n",
    "data = preprocess(data)\n",
    "data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T09:10:22.231200Z",
     "start_time": "2021-02-17T09:10:21.265193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['aktiva', ':', 'umlaufvermögen', ':', '25.023,63', '|', 'aktiva', ':', 'umlaufvermögen', ':', 'forderungen', 'und', 'sonstige', 'vermögensgegenstände', ':', '67,28', '|', 'aktiva', ':', 'umlaufvermögen', ':', 'kassenbestand', 'bundesbankguthaben', 'guthaben', 'bei', 'kreditinstituten', 'und', 'schecks', ':', '276,50', '|', 'aktiva', ':', 'total', ':', '31.658,20', '||', 'passiva', ':', 'eigenkapital', ':', '13.161,23', '|', 'passiva', ':', 'eigenkapital', ' ', ':', 'gezeichnetes', 'kapital', ':', '25.000,00', '|', 'passiva', ':', 'eigenkapital', ' ', ':', 'jahresfehlbetrag', ':', '810,75', '|', 'passiva', ':', 'rückstellungen', ':', '400,00', '|', 'passiva', ':', 'verbindlichkeiten', ':', '434,38', '|', 'passiva', ':', 'total', ':', '25.023,63', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(data, field_, field_, model, device, max_len = MAX_LEN)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "ru",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ru",
   "useGoogleTranslate": true
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
